{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis exploratorio y curación de datos\n",
    "\n",
    "En ésta entrega se comenzará a preparar el dataset para los prácticos siguientes. Algunos puntos de aquí ya han sido cubiertos en la entrega anterior para hacer el análisis. Otros pueden tener un tratamiento especial según nuestro enfoque de NLP.\n",
    "\n",
    "Listado de puntos a cubrir en análisis y curación:\n",
    "- a. Asegurarse de tener ids/claves únicas\n",
    "- b. Chequear que no haya datos duplicados\n",
    "- c. No usar caracteres especiales en las etiquetas de variables\n",
    "- d. Verificar que no haya problemas de codificación/encoding\n",
    "- e. Verificar la consistencia de las variables\n",
    "- f. Identificar y documentar valores atípicos/outliers. Qué outliers pueden haberse encontrado en el análisis del práctico previo?\n",
    "- g. Tratar valores faltantes: quitar o imputar. En éste caso, se correspondería tambien con buscar si hay conversaciones vacías, y quitarlas ya que no aportarían nada a los prácticos siguientes \n",
    "- h. Codificar variables: las variables categóricas deben ser etiquetadas como variables numéricas, no como cadenas. Aquí ya se puede empezar a pensar en vectorizar las frases para los prácticos posteriores. Explore CountVectorizer y TdidfVectorizer de sklearn (links abajo). Así mismo, considere si corresponde algún preprocesamiento de las frases antes de la vectorización.\n",
    "\n",
    "Con los puntos anteriores ya se puede preparar los datasets que vamos a usar más adelante.\n",
    "\n",
    "DATASET1- Preparar un dataset para usar modelos supervisados. Éste debe incluir las frases de usuario, que son las que se van a usar para clasificar, más los labels.\n",
    "\n",
    "DATASET2- Preparar el dataset para el práctico de aprendizaje no supervisado. Éste está orientado a descubrir tópicos en las preguntas no respondidas por el asistente, por lo que en éste caso debe filtrar éste subconjunto de frases.\n",
    "\n",
    "Por último, guarde los datasets en un formato conveniente para usar después. Importante: guardarlos en nuevos archivos para no sobreescribir los datos crudos u originales.\n",
    "\n",
    "OPCIONAL: en el punto h. donde hay que vectorizar las frases de usuario, también se puede probar con word embeddings preentrenados. Como en éste caso se trata de oraciones, habría que usar los embeddings de palabras para los tokens y combinarlos para tener una representación por cada frase. Se puede pensar en la suma de los vectores o el promedio por ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Links de inteŕes:__\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "* https://moj-analytical-services.github.io/NLP-guidance/\n",
    "* https://es.wikipedia.org/wiki/Tf-idf\n",
    "\n",
    "__Palabras clave:__\n",
    "* matríz de frecuencias de documentos términos (countvectorizer en sklearn)\n",
    "* Tfidf- term frecuency- inverse document frecuency\n",
    "* LSA- análisis semántico latente\n",
    "* SVD- descomposición de valores singulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
